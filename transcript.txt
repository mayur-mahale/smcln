everyone lets get started with icecream call today updates from my and include i was working on planning for semi colons and distributing the task between the team players  i also coordinated with the it ag team to see a bit laptops  another infrastructure needs which we would have so lets go round table quickly  but then why dont you start  this was mostly contact number 53  it was keyboard detection  so  physically month  data comes into the bank and we need people detection  mostly because of did for topic  modelling and stuff     so what i did was it tested around 34 api is so they was like microsoft  ibm watson  rick and google  so aibe limited all the all the score  and now we can actually tell a tick the best of them  so i will also working on task 82  which was the high priority task  so it was about speaker  recognition  ive already implemented microsoft  speaker  speaker recognition  so we just have to integrate it with our google speech data and solve ok  so so my name  so i am working on december 10  which is the preprocessing of the transcripts  so that includes removing of the stopper  remember the punctuation  so i am using nltk library for that thing and also exploring spacey for that  also  along with that torsten  i am also working on task number 11  which is sex    that sentence segmentation  i am using the penalty ke sentence  tokenizer to state of the sentences for normal  translate  able to separate out two sentences into an array  but have to check for other conditions like if there are large whitespace characters or if there are any kind of a line feed on carriage return that just check how it behave so thats it from my bed hi  so i was given task 74  which is headline generator  so when each person speaks  they will be given this be a brief summary of sentences of what they have been denied to generate a headline to show to cover the whole issue up  so i had a check  3  github  repos and who won the results very good with the model was very large to train which give issues  another was an abstraction summarise of which the result wasnt that good  then we also found tensorflow model  which is already pretrained    however  it is not  it has not fit our data  so we may have to try fitting it without data first and then see the results  i was also given task 75  which is bigger  generating a summary from the whole content  get a brief summary of what has happened so far  this we used alien api  which has a very good  generate very good summary  and it has also other features like entity  extraction and taxonomy and all so i want to pictures  i have a dependency on burden because i need to translate so if you can speed up his thing and i can get the real life transcripts coming into my preprocessing and sentences notation of api  so it will be good for me  i forgot to do actually was i    i had a few questions about the task number 10  which was given to mayur mayur  can you tell me how exactly is nltk going to help us with preprocessing ? have you compared nltk with pc that i am currently assessing the options ? i will be evaluating base like a no in terms of memory  requirement their timing and also be because it will improve the efficiency of a application it  so i am comparing both and whichever is the best i will be using that yam hai  this is water  you can we can you tell me more about prepossessing ? are you like ? removing stock version been standing on all the stuff yeah ? i am doing that  but if it is required  if you dont want those things you can  let me know what i can do it    i can write a kind of otc provide functionality where you can ask whether you want to stop word removal punctuation when its processing the transcript life  so i can provide this functionality  i can provide both the things  remove  stopwords ramola punctuation everything  but if you want to skip it  i will provide a dogs mouth  you had a question in preprocessing  sometimes speech to text doesnt  give such good results right  suryavamsam explicit work  not needed or not said  so how would be block that out ? so for that ? what up what i am thinking of his first  i will look for sentence correction in case  if  if i am understanding correctly  i will go for sentence correction also  i will be using a chance im  also or pc  to get or understand entities to get the entities and see if it is a valid entity or not - maybe sometimes we username or noun  which is not there in the vector space  so we can remove it so that it doesnt create any kind of issue    ok  so top 10 seems to be a very high priority  one with deals with three processing  so lets give some more attention to this task and alright  so thanks for all your updates  thank you  saath mein